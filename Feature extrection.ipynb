{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRUm8Wk91dbeKk1GOcwnGK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# `Task : Text preprocessing `"],"metadata":{"id":"c17aZwIHOIlW"}},{"cell_type":"markdown","source":["**Import Libraries**\n","\n","nltk, re, and NLP tools (stopwords, tokenizers, stemmer, lemmatizer, POS tagger).\n","\n","**Download NLTK Resources**\n","\n","Ensures required datasets (stopwords, tokenizer, POS models, WordNet) are available.\n","\n","**Raw Text Input**\n","\n","A sample paragraph about NLP is used.\n","\n","**Lowercasing**\n","\n","Converts all text into lowercase → makes text uniform.\n","\n","**Tokenization**\n","\n","Splits text into words/tokens (e.g., “NLP stands for” → [\"nlp\", \"stands\", \"for\"]).\n","\n","**Remove Punctuation & Numbers**\n","\n","Keeps only alphabetic words using .isalpha().\n","\n","**Stopword Removal**\n","\n","Removes common words (like is, and, the) to keep meaningful words.\n","\n","Stemming (PorterStemmer)\n","\n","Reduces words to root form (running → run, machines → machin).\n","\n","Lemmatization (WordNetLemmatizer)\n","\n","Converts words to dictionary form (better → good, machines → machine).\n","\n","**POS Tagging**\n","Assigns grammatical roles (e.g., nlp/NN, stands/VBZ)."],"metadata":{"id":"94n1nejwPZj3"}},{"cell_type":"code","source":[" import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.tag import pos_tag\n","import re"],"metadata":{"id":"C9UnZ1tVHy-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#`Download `"],"metadata":{"id":"zG7aDhiHRIaD"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfJeshPXRF5V","executionInfo":{"status":"ok","timestamp":1758609044090,"user_tz":-300,"elapsed":1040,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"aa6c736a-9941-42b7-90f1-fa15e0cbdd40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# `Raw text provided`"],"metadata":{"id":"G95qx53TQHMN"}},{"cell_type":"code","source":["\n","raw_text = \"\"\" 'NLP stands for Natural Language Processing, a branch of artificial intelligence (AI)\n","and computer science focused on enabling computers to understand, interpret, and generate\n","human language in both text and speech. By combining computer science and linguistics,\n","NLP allows machines to analyze vast amounts of unstructured language data from sources like\n","emails and social media, powering applications such as chatbots, translation services,\n","voice assistants, and sentiment analysis tools.' \"\"\""],"metadata":{"id":"EPwF92nsJv2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Original Text:\\n\", raw_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmNIEjAWKX5C","executionInfo":{"status":"ok","timestamp":1758607314867,"user_tz":-300,"elapsed":23,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"95b2d022-ac6d-4663-e3f5-51cf637ac0cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text:\n","  'NLP stands for Natural Language Processing, a branch of artificial intelligence (AI)\n","and computer science focused on enabling computers to understand, interpret, and generate\n","human language in both text and speech. By combining computer science and linguistics,\n","NLP allows machines to analyze vast amounts of unstructured language data from sources like\n","emails and social media, powering applications such as chatbots, translation services,\n","voice assistants, and sentiment analysis tools.' \n"]}]},{"cell_type":"markdown","source":["# ` Step 1: Lowercasing`"],"metadata":{"id":"QHLOoLHrQTJl"}},{"cell_type":"code","source":["\n","text_lower = raw_text.lower()\n","print(\"\\nLowercased Text:\\n\", text_lower)\n","nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Xv5twfMKiOx","executionInfo":{"status":"ok","timestamp":1758607346378,"user_tz":-300,"elapsed":255,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"31cb3fa9-c2f1-43e8-c3f3-be7a2c496165"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Lowercased Text:\n","  'nlp stands for natural language processing, a branch of artificial intelligence (ai)\n","and computer science focused on enabling computers to understand, interpret, and generate\n","human language in both text and speech. by combining computer science and linguistics,\n","nlp allows machines to analyze vast amounts of unstructured language data from sources like\n","emails and social media, powering applications such as chatbots, translation services,\n","voice assistants, and sentiment analysis tools.' \n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# `Step 2: Tokenization`"],"metadata":{"id":"qkk5xXu1QbRd"}},{"cell_type":"code","source":["\n","tokens = word_tokenize(text_lower)\n","print(\"\\nTokenized Words:\\n\", tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEY9HM5iKqcE","executionInfo":{"status":"ok","timestamp":1758607385372,"user_tz":-300,"elapsed":172,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"b70eb9e2-8ad7-42f4-92ce-5749bce3e156"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tokenized Words:\n"," [\"'nlp\", 'stands', 'for', 'natural', 'language', 'processing', ',', 'a', 'branch', 'of', 'artificial', 'intelligence', '(', 'ai', ')', 'and', 'computer', 'science', 'focused', 'on', 'enabling', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'both', 'text', 'and', 'speech', '.', 'by', 'combining', 'computer', 'science', 'and', 'linguistics', ',', 'nlp', 'allows', 'machines', 'to', 'analyze', 'vast', 'amounts', 'of', 'unstructured', 'language', 'data', 'from', 'sources', 'like', 'emails', 'and', 'social', 'media', ',', 'powering', 'applications', 'such', 'as', 'chatbots', ',', 'translation', 'services', ',', 'voice', 'assistants', ',', 'and', 'sentiment', 'analysis', 'tools', '.', \"'\"]\n"]}]},{"cell_type":"markdown","source":["# `Step 3: Remove punctuation and numbers`"],"metadata":{"id":"FEomoDEcQf-L"}},{"cell_type":"code","source":["\n","tokens = [word for word in tokens if word.isalpha()]\n","print(\"\\nAfter Removing Punctuation/Numbers:\\n\", tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8r1Lw4hK6T8","executionInfo":{"status":"ok","timestamp":1758607448476,"user_tz":-300,"elapsed":372,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"4e383b8f-8b77-4438-c39e-708a5cedc349"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After Removing Punctuation/Numbers:\n"," ['stands', 'for', 'natural', 'language', 'processing', 'a', 'branch', 'of', 'artificial', 'intelligence', 'ai', 'and', 'computer', 'science', 'focused', 'on', 'enabling', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'both', 'text', 'and', 'speech', 'by', 'combining', 'computer', 'science', 'and', 'linguistics', 'nlp', 'allows', 'machines', 'to', 'analyze', 'vast', 'amounts', 'of', 'unstructured', 'language', 'data', 'from', 'sources', 'like', 'emails', 'and', 'social', 'media', 'powering', 'applications', 'such', 'as', 'chatbots', 'translation', 'services', 'voice', 'assistants', 'and', 'sentiment', 'analysis', 'tools']\n"]}]},{"cell_type":"markdown","source":["# `Step 4: Remove Stopwords`"],"metadata":{"id":"amKaYR4HRado"}},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word not in stop_words]\n","print(\"\\nAfter Stopword Removal:\\n\", filtered_tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0iVyN5DLCNf","executionInfo":{"status":"ok","timestamp":1758609114773,"user_tz":-300,"elapsed":36,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"1d453a7d-c685-418e-8ddf-143b66876d96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After Stopword Removal:\n"," ['stands', 'natural', 'language', 'processing', 'branch', 'artificial', 'intelligence', 'ai', 'computer', 'science', 'focused', 'enabling', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'text', 'speech', 'combining', 'computer', 'science', 'linguistics', 'nlp', 'allows', 'machines', 'analyze', 'vast', 'amounts', 'unstructured', 'language', 'data', 'sources', 'like', 'emails', 'social', 'media', 'powering', 'applications', 'chatbots', 'translation', 'services', 'voice', 'assistants', 'sentiment', 'analysis', 'tools']\n"]}]},{"cell_type":"markdown","source":["# ` Step 5: Stemming `"],"metadata":{"id":"oiQd-rSyRfpU"}},{"cell_type":"code","source":["\n","stemmer = PorterStemmer()\n","stemmed = [stemmer.stem(word) for word in filtered_tokens]\n","print(\"\\nAfter Stemming:\\n\", stemmed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgaUlb1KLMnJ","executionInfo":{"status":"ok","timestamp":1758609140149,"user_tz":-300,"elapsed":75,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"4c8957d8-fa9b-4552-e51c-40a212e0b2f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After Stemming:\n"," ['stand', 'natur', 'languag', 'process', 'branch', 'artifici', 'intellig', 'ai', 'comput', 'scienc', 'focus', 'enabl', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'text', 'speech', 'combin', 'comput', 'scienc', 'linguist', 'nlp', 'allow', 'machin', 'analyz', 'vast', 'amount', 'unstructur', 'languag', 'data', 'sourc', 'like', 'email', 'social', 'media', 'power', 'applic', 'chatbot', 'translat', 'servic', 'voic', 'assist', 'sentiment', 'analysi', 'tool']\n"]}]},{"cell_type":"markdown","source":["# `Step 6: Lemmatization`"],"metadata":{"id":"MNHyh6pMRmj6"}},{"cell_type":"code","source":["\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n","print(\"\\nAfter Lemmatization:\\n\", lemmatized)\n","import nltk\n","nltk.download('averaged_perceptron_tagger_eng')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z2ms9z_PLSum","executionInfo":{"status":"ok","timestamp":1758609185066,"user_tz":-300,"elapsed":3320,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"85a6c732-f6f6-408d-ef34-fb5f9c88fd43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After Lemmatization:\n"," ['stand', 'natural', 'language', 'processing', 'branch', 'artificial', 'intelligence', 'ai', 'computer', 'science', 'focused', 'enabling', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'text', 'speech', 'combining', 'computer', 'science', 'linguistics', 'nlp', 'allows', 'machine', 'analyze', 'vast', 'amount', 'unstructured', 'language', 'data', 'source', 'like', 'email', 'social', 'medium', 'powering', 'application', 'chatbots', 'translation', 'service', 'voice', 'assistant', 'sentiment', 'analysis', 'tool']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["# `Step 7: POS Tagging`"],"metadata":{"id":"_xWqe5jXRs2C"}},{"cell_type":"code","source":["pos_tags = pos_tag(filtered_tokens)\n","print(\"\\nPOS Tagging:\\n\", pos_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOtWZT2KLhXQ","executionInfo":{"status":"ok","timestamp":1758609195390,"user_tz":-300,"elapsed":182,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"4dbb8385-27f3-4e94-cd35-349b4acb1256"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","POS Tagging:\n"," [('stands', 'NNS'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('branch', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('ai', 'VBP'), ('computer', 'NN'), ('science', 'NN'), ('focused', 'VBD'), ('enabling', 'VBG'), ('computers', 'NNS'), ('understand', 'VBP'), ('interpret', 'JJ'), ('generate', 'NN'), ('human', 'JJ'), ('language', 'NN'), ('text', 'NN'), ('speech', 'NN'), ('combining', 'VBG'), ('computer', 'NN'), ('science', 'NN'), ('linguistics', 'NNS'), ('nlp', 'JJ'), ('allows', 'NNS'), ('machines', 'NNS'), ('analyze', 'VBP'), ('vast', 'JJ'), ('amounts', 'NNS'), ('unstructured', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('sources', 'NNS'), ('like', 'IN'), ('emails', 'NNS'), ('social', 'JJ'), ('media', 'NNS'), ('powering', 'VBG'), ('applications', 'NNS'), ('chatbots', 'VBP'), ('translation', 'NN'), ('services', 'NNS'), ('voice', 'NN'), ('assistants', 'NNS'), ('sentiment', 'JJ'), ('analysis', 'NN'), ('tools', 'NNS')]\n"]}]},{"cell_type":"markdown","source":["#  `Q2  Vocabulary & Bag of Words `"],"metadata":{"id":"ibcHqvieRxh9"}},{"cell_type":"markdown","source":["# `Code Description`"],"metadata":{"id":"pRMIKt7NW-ox"}},{"cell_type":"markdown","source":["**Goal:** Convert raw text into a numerical Bag-of-Words (BoW) representation.\n","\n","**Steps:**\n","\n","**Preprocess text :** lowercase, remove symbols, tokenize, remove stopwords.\n","\n","**Build vocabulary :** unique sorted words from the corpus.\n","\n","**Create BoW matrix :** count frequency of each vocabulary word in each sentence.\n"],"metadata":{"id":"0xn4F5H1XFwq"}},{"cell_type":"code","source":["import nltk, re, pandas as pd\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter"],"metadata":{"id":"aDkK-8EAMFgY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `Corpus`\n"],"metadata":{"id":"OJuD2w0BSBwq"}},{"cell_type":"code","source":["\n","corpus = [\n","    \"I am loving the NLP class, but sometimes it feels confusing!!!\",\n","    \"NLP is a fascinating field — it deals with text, speech, and language understanding.\"\n","]\n","def generate_vocabulary_bow(corpus):\n","    stop_words = set(stopwords.words('english'))\n","    processed = []"],"metadata":{"id":"nYMljXPkMWNT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Preprocessing:** lowercase, clean, tokenize, remove stopwords"],"metadata":{"id":"C4F-__yzSnRb"}},{"cell_type":"code","source":[" for sentence in corpus:\n","        sentence = re.sub(r'[^a-z\\s]', '', sentence.lower())\n","        tokens = [w for w in word_tokenize(sentence) if w not in stop_words]\n","        processed.append(tokens)\n"],"metadata":{"id":"P16TQkO0MjUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `vocabulary `"],"metadata":{"id":"kUygGA6jUy9M"}},{"cell_type":"code","source":["vocabulary = sorted({word for sent in processed for word in sent})"],"metadata":{"id":"iZEsHBsVU1kb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `BOW Matrix`"],"metadata":{"id":"1RbPObeLVFE6"}},{"cell_type":"code","source":["bow_matrix = [[Counter(sent).get(word, 0) for word in vocabulary] for sent in processed]"],"metadata":{"id":"TJr1-aNpVKj0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `Output`"],"metadata":{"id":"awzbLsiGVeMM"}},{"cell_type":"code","source":["print(\"Vocabulary:\\n\", vocabulary, \"\\n\")\n","print(\"Bag of Words Matrix:\\n\")\n","print(pd.DataFrame(bow_matrix, columns=vocabulary))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRWLGyj9VkW4","executionInfo":{"status":"ok","timestamp":1758610298885,"user_tz":-300,"elapsed":27,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"1504b051-11cd-4298-af0b-a84383ba1a5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," ['class', 'confusing', 'deals', 'fascinating', 'feels', 'field', 'language', 'loving', 'nlp', 'sometimes', 'speech', 'text', 'understanding'] \n","\n","Bag of Words Matrix:\n","\n","   class  confusing  deals  fascinating  feels  field  language  loving  nlp  \\\n","0      1          1      0            0      1      0         0       1    1   \n","1      0          0      1            1      0      1         1       0    1   \n","2      1          1      0            0      1      0         0       1    1   \n","3      0          0      1            1      0      1         1       0    1   \n","4      1          1      0            0      1      0         0       1    1   \n","5      0          0      1            1      0      1         1       0    1   \n","6      1          1      0            0      1      0         0       1    1   \n","7      0          0      1            1      0      1         1       0    1   \n","\n","   sometimes  speech  text  understanding  \n","0          1       0     0              0  \n","1          0       1     1              1  \n","2          1       0     0              0  \n","3          0       1     1              1  \n","4          1       0     0              0  \n","5          0       1     1              1  \n","6          1       0     0              0  \n","7          0       1     1              1  \n"]}]},{"cell_type":"code","source":["import math\n","import pandas as pd\n","from collections import Counter\n","import re"],"metadata":{"id":"W4cNVplMqNQ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ` Input text `"],"metadata":{"id":"w06ZtkaczYtI"}},{"cell_type":"code","source":["text = \"\"\"NLP stands for Natural Language Processing, a branch of artificial intelligence (AI)\n","and computer science focused on enabling computers to understand, interpret, and generate\n","human language in both text and speech. By combining computer science and linguistics,\n","NLP allows machines to analyze vast amounts of unstructured language data from sources like\n","emails and social media, powering applications such as chatbots, translation services,\n","voice assistants, and sentiment analysis tools.\"\"\""],"metadata":{"id":"Dth7OtpmqW6Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  `Split text into \"documents\" `"],"metadata":{"id":"K-KPgnRMzsTT"}},{"cell_type":"code","source":["docs = re.split(r'[.!?]', text)   # split by sentence end\n","docs = [d.strip() for d in docs if d.strip()]   # remove empty"],"metadata":{"id":"kdD6hLOjqjdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  `Tokenize `"],"metadata":{"id":"VlpsCyQr1KKH"}},{"cell_type":"code","source":["\n","tokenized_docs = []\n","for d in docs:\n","    tokens = re.findall(r'\\b\\w+\\b', d.lower())\n","    tokenized_docs.append(tokens)\n"],"metadata":{"id":"5dnNNUFEx5OU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ` Tokenize (lowercase, keep only words)`"],"metadata":{"id":"EbGjGb5e1dkq"}},{"cell_type":"code","source":["tokenized_docs = []\n","for d in docs:\n","    tokens = re.findall(r'\\b\\w+\\b', d.lower())\n","    tokenized_docs.append(tokens)\n"],"metadata":{"id":"UwVu9eZCx_1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `Total number of documents`"],"metadata":{"id":"GWxI2ERy1oV_"}},{"cell_type":"code","source":["N = len(tokenized_docs)"],"metadata":{"id":"A_aD9emG8eoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ` Build Vocabulary & Counters `"],"metadata":{"id":"7U5UbZoh8vgR"}},{"cell_type":"code","source":["\n","counters = [Counter(doc) for doc in tokenized_docs]\n","vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n"],"metadata":{"id":"iYUwLpC8yEFS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `Compute IDF`"],"metadata":{"id":"-yXsJG-E876y"}},{"cell_type":"code","source":["df = {term: sum(1 for c in counters if c[term] > 0) for term in vocab}\n","idf = {term: math.log(N / df[term]) if df[term] > 0 else 0 for term in vocab}"],"metadata":{"id":"tY_VEK5VyTIP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ` Compute TF (normalized) and TF–IDF`"],"metadata":{"id":"i41SWM-x9KGz"}},{"cell_type":"code","source":["rows = []\n","for term in vocab:\n","    row = {\"term\": term, \"idf\": round(idf[term], 6)}\n","    for i, c in enumerate(counters, 1):\n","        tf_raw = c[term]\n","        total_terms = sum(c.values())\n","        tf_norm = tf_raw / total_terms if total_terms > 0 else 0\n","        tfidf = tf_norm * idf[term]\n","\n","        row[f\"tf_doc{i}\"] = tf_raw\n","        row[f\"tf_norm_doc{i}\"] = round(tf_norm, 6)\n","        row[f\"tfidf_doc{i}\"] = round(tfidf, 6)\n","    rows.append(row)"],"metadata":{"id":"Wne-auTNyf8k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  `Create TF–IDF Table`"],"metadata":{"id":"UPiOx6Sf9YQQ"}},{"cell_type":"code","source":["df_table = pd.DataFrame(rows)\n","cols = [\"term\", \"idf\"]\n","for i in range(1, N+1):\n","    cols += [f\"tf_doc{i}\", f\"tf_norm_doc{i}\", f\"tfidf_doc{i}\"]\n","df_table = df_table[cols]"],"metadata":{"id":"RygOcxo_yka-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ` Display first 20 rows `"],"metadata":{"id":"sz4Ghl9b9t46"}},{"cell_type":"code","source":["\n","print(df_table.head(20))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV5iqGXdyphN","executionInfo":{"status":"ok","timestamp":1758953395915,"user_tz":-300,"elapsed":341,"user":{"displayName":"Ali Ejaz","userId":"07742504463080000261"}},"outputId":"21a7a6d5-e9ba-4948-ff30-87501beb3de6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["            term       idf  tf_doc1  tf_norm_doc1  tfidf_doc1  tf_doc2  \\\n","0              a  0.693147        1      0.032258     0.02236        0   \n","1             ai  0.693147        1      0.032258     0.02236        0   \n","2         allows  0.693147        0      0.000000     0.00000        1   \n","3        amounts  0.693147        0      0.000000     0.00000        1   \n","4       analysis  0.693147        0      0.000000     0.00000        1   \n","5        analyze  0.693147        0      0.000000     0.00000        1   \n","6            and  0.000000        3      0.096774     0.00000        3   \n","7   applications  0.693147        0      0.000000     0.00000        1   \n","8     artificial  0.693147        1      0.032258     0.02236        0   \n","9             as  0.693147        0      0.000000     0.00000        1   \n","10    assistants  0.693147        0      0.000000     0.00000        1   \n","11          both  0.693147        1      0.032258     0.02236        0   \n","12        branch  0.693147        1      0.032258     0.02236        0   \n","13            by  0.693147        0      0.000000     0.00000        1   \n","14      chatbots  0.693147        0      0.000000     0.00000        1   \n","15     combining  0.693147        0      0.000000     0.00000        1   \n","16      computer  0.000000        1      0.032258     0.00000        1   \n","17     computers  0.693147        1      0.032258     0.02236        0   \n","18          data  0.693147        0      0.000000     0.00000        1   \n","19        emails  0.693147        0      0.000000     0.00000        1   \n","\n","    tf_norm_doc2  tfidf_doc2  \n","0       0.000000    0.000000  \n","1       0.000000    0.000000  \n","2       0.027027    0.018734  \n","3       0.027027    0.018734  \n","4       0.027027    0.018734  \n","5       0.027027    0.018734  \n","6       0.081081    0.000000  \n","7       0.027027    0.018734  \n","8       0.000000    0.000000  \n","9       0.027027    0.018734  \n","10      0.027027    0.018734  \n","11      0.000000    0.000000  \n","12      0.000000    0.000000  \n","13      0.027027    0.018734  \n","14      0.027027    0.018734  \n","15      0.027027    0.018734  \n","16      0.027027    0.000000  \n","17      0.000000    0.000000  \n","18      0.027027    0.018734  \n","19      0.027027    0.018734  \n"]}]}]}